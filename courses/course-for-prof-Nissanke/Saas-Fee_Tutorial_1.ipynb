{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emvgHmhCmZJv"
   },
   "source": [
    "# Gravitational Wave Data Analysis 2: Characterising Signals\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "In this notebook we will use the gravitational wave data analysis toolkit called `PyCBC` to download, read and process real gravitational wave data.\n",
    "\n",
    "In this tutorial we will look at how to interpret a candidate signal via Bayesian analysis tools available to us. This tutorial is based on tutorials from the [Gravitational Wave Open Data Workshop](https://github.com/gw-odw) and [`PyCBC` Tutorials](https://github.com/gwastro/PyCBC-Tutorials) repositories, taking code and inspiration heavily from both. I recommend looking at these resources for other examples as we will not cover all of the same things here. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKgf-2PsjmFU"
   },
   "source": [
    "## 0. Setting up the environment\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "If you're using this notebook inside the Renku environment all the required packages should be installed already, and you can leave the commented line as it is in the next block. \n",
    "    You can also run this notebook outside the Renku environment, but then we need to make sure all the correct packages are installed. The notebook can either be run on a remote server in Google Collaboratory, but you can also download to your own machine, so long as you have a working `python` installation with all the required packages.\n",
    "\n",
    "We want to have `PyCBC` and all its dependencies. These include the `lalsuite` and `ligo-common` packages, which contain a lot of the underlying code for handling the data and generating model gravitational wave signals.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEraQYeLjmFV",
    "outputId": "b3f5aea0-a5f3-4ede-f922-6343e3070c3d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install pycbc==1.17.0 lalsuite ligo-common --no-cache-dir\n",
    "from IPython.display import Math\n",
    "from matplotlib import pyplot as plt, rc, cycler\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "palette[3], palette[5] = palette[5], palette[3]\n",
    "rc(\"axes\", prop_cycle=cycler(color=palette))\n",
    "alpha=0.5\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuGUtVTtjmFc"
   },
   "source": [
    "## 1. Models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "A `model` in pycbc inference represents the problem you are trying to solve. It contains the definition of the likelihood function you want to explore and details the parameters you are using. In this section, we'll walk through using models with pycbc inference and see how to create your own.\n",
    "\n",
    "Let's see what models are available. All models are accessible via a dictionary in the `models` module. Each model has a unique name:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYAeSjUsjmFd",
    "outputId": "b7ddf8a6-3034-46e7-d637-167d7de73e7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyCBC.libutils: pkg-config call failed, setting NO_PKGCONFIG=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_eggbox\n",
      "test_normal\n",
      "test_rosenbrock\n",
      "test_volcano\n",
      "test_posterior\n",
      "test_prior\n",
      "gaussian_noise\n",
      "marginalized_phase\n",
      "marginalized_polarization\n",
      "marginalized_hmpolphase\n",
      "brute_parallel_gaussian_marginalize\n",
      "gated_gaussian_noise\n",
      "gated_gaussian_margpol\n",
      "single_template\n",
      "relative\n"
     ]
    }
   ],
   "source": [
    "from pycbc.inference import models\n",
    "\n",
    "for model_name in models.models:\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIvx-TWKjmFf"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "The models starting with `test_` are analytic models. These have predefined likelihood functions that are given by some standard distributions used in testing samplers. The other models are for gravitational-wave astronomy: they take in data and calculate a likelihood using an inner product between the data and a signal model. Currently, all of the gravitational-wave models in PyCBC assume that the data is stationary Gaussian noise in the absence of a signal. The difference between the models is they make varying simplfying assumptions, in order to speed up likelihood evaluation.\n",
    "\n",
    "Below, we'll start with an analytic model to illustrate some basics of how PyCBC Inference works. We'll then use one of the simplified models to quickly estimate some parameters of GW170817.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_JjuLQWjmFh"
   },
   "source": [
    "### 1.1 One-dimensional Analytic Model ###\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "The simplest case is a problem with a single parameter. We'll go through how to estimate this parameter using an analytic model. In this case, we'll use the normal distribution.\n",
    "\n",
    "Create an instance of a pre-made Model. This is an analytic model (i.e. no data used)\n",
    "that we employ largely for testing the capabilities of different samplers. \n",
    "This will create a likelihood surface in one dimensions (x) with zero mean and unit variance\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r3BoTL1WjmFj"
   },
   "outputs": [],
   "source": [
    "from pycbc.inference import models, sampler\n",
    "from pycbc import distributions as dists\n",
    "\n",
    "my_model = models.TestNormal(('x'), mean=(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nThHbaACjmFk"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "We need to choose a sampler, in this case *emcee*. We need to provide the model we are using along with the prior and number of walkers. Emcee is an 'ensemble' sampler so it consists of many points which are traversing the space and help each other explore the likelihood surface.\n",
    "\n",
    "Before we start we need to decide the initial positions of the walkers. In this case we choose that they be distributed randomly between -1 and 1. We use the 'Uniform' distribution class. It is a common feature that these classes take the parameter name along with parameters that may define the distribution itself (such as bounds and other distribution-specific shape determining variables).\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VDdhggoFjmFl"
   },
   "outputs": [],
   "source": [
    "from pycbc.inference.sampler import emcee\n",
    "engine = sampler.emcee.EmceeEnsembleSampler(my_model, nwalkers=1000, nprocesses=1)\n",
    "_ = engine.set_p0(prior=(dists.Uniform(x=(-1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLn9mJUvjmFq"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  Run the mcmc for 200 iterations </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mTvzDhKXjmFq"
   },
   "outputs": [],
   "source": [
    "engine.run_mcmc(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZapaVcvjmFr"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "We can get the entire history of where the \"walkers\" have been by looking at the samples\n",
    "attribute. For each variable parameter, we get an array with \n",
    "dimensions nwalkers x num_iterations. This is the format for the 'Emcee' sampler. Other samples may have other formats for their parameter chains. For example, parallel tempered samplers will have an additional dimension which represents the temperature. The chain has 2 dimensions, the first axis is the walker\n",
    "and the second is the iteration. We'll plot the final position of each walker\n",
    "     </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "GIwpCvp7jmFs",
    "outputId": "2df52b5e-0ebc-4c7c-94b2-74cebc3ca7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean = -0.019; variance = 1.021\n"
     ]
    }
   ],
   "source": [
    "xchain = engine.samples['x']\n",
    "values = xchain[:,-1]  \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.hist(values, density=True)\n",
    "ax.set_xlabel('x')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"Estimated mean = {:.3f}; variance = {:.3f}\".format(values.mean(), np.var(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_AzyF90jmFu"
   },
   "source": [
    "####  Suggested Problem ####\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Try changing the mean of the analytic distribution. We've provided a random number below (no peaking!) What do you estimate for the mean of the distribution? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VrCmKseAjmFv"
   },
   "outputs": [],
   "source": [
    "from numpy.random import uniform, seed\n",
    "seed(0)\n",
    "a_number = uniform(-100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avtlTgf8jmFw"
   },
   "source": [
    "### 1.2 Using a model with priors ###\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "You may have noticed that in the previous example we did not specify any prior. If you do not specify a prior, a flat prior (with no boundaries) will be used. Let's now specify a prior. To do so, we will use the [distributions](http://pycbc.org/pycbc/latest/html/pycbc.distributions.html#pycbc-distributions-package) package. This contains many pre-made pdfs which can be used to build a prior. In fact we've been using this already to set initial walker positions.\n",
    "\n",
    "Let's create a prior on our variable that is uniform between $[0, 10)$. The resulting posterior should be a truncated normal distribution. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X5WFYU5AjmFx"
   },
   "outputs": [],
   "source": [
    "prior_x = dists.Uniform(x=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV5DYK-2jmFz"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "In order to provide the prior to the model, we need to wrap it in a [JointDistribution](http://pycbc.org/pycbc/latest/html/pycbc.distributions.html#pycbc.distributions.joint.JointDistribution). The JointDistribution provides a common API for the models to evaluate the prior at a given point. If we had a problem with multiple parameters, each with their own prior distribution, we could provide the different distributions `JointDistribution`; it will take the product over all the distributions, providing the model a single number for the prior pdf. The `JointDistribution` also allows you to provide arbitrary constraints on parameters, although we will not cover that here. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1_B9a3MjmF1"
   },
   "outputs": [],
   "source": [
    "prior = dists.JointDistribution(['x'], prior_x)\n",
    "\n",
    "# Prior is a standard keyword that models which inherit from BaseModel \n",
    "# can take.\n",
    "my_model = models.TestNormal('x', prior=prior)\n",
    "\n",
    "engine = sampler.EmceeEnsembleSampler(my_model, nwalkers=1000, nprocesses=8)\n",
    "\n",
    "# Note that we do not need to provide anything to `set_p0` to set the initial positions\n",
    "# this time. By default, the sampler will draw from the prior in this case.\n",
    "engine.set_p0()\n",
    "engine.run_mcmc(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vjGsUWXjmF1"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "Since the `TestNormal` model's likelihood has zero mean and unit variance, forcing $x$ to only accept positive values via the prior means that our posterior should be a $\\chi$ distribution with 1 degree of freedom. Let's check: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZ7WC28DjmF4",
    "outputId": "8e904024-4ae2-46d2-814c-2ffa022e0927"
   },
   "outputs": [],
   "source": [
    "xchain = engine.samples['x']\n",
    "values = xchain[:,-1]  \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.hist(values, density=True, label='measured')\n",
    "\n",
    "from scipy import stats as spst\n",
    "xpts = np.linspace(0, 3, num=100)\n",
    "y = spst.chi.pdf(xpts, 1.)\n",
    "ax.plot(xpts, y, label='$\\chi$ dist. with 1 d.o.f.')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txpeWsjwjmGG"
   },
   "source": [
    "## 2. Estimating the distance of GW170817 ##\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "Now that we have some experience with models in pycbc inference, let's take a look at some of the existing models that pycbc inference provides targeted at gravitational-wave data analysis. We'll start with the `SingleTemplate` model. This model is useful when we know the intrinsic parameters of a source (i.e. component masses, spins), but we don't know the extrinsic parameters (i.e. sky location, distance, binary orientation). This will allow us to estimate the distance to GW170817 and the inclination angle (angle between the orbital angular momentum and our line-of-sight). \n",
    "\n",
    "This model requires a specifc set of data products.\n",
    "  * Dictionary of frequency-domain data (keyed by observatory short name such as 'H1', 'L1', 'V1').\n",
    "  * Dictionary of power spectral density estimates\n",
    "  * Low frequency cutoff to use for internal filtering in the model\n",
    "  \n",
    "We will make use of PyCBC gw signal processing tools to prepare this data. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEQyiKdajpbu",
    "outputId": "94dca06a-e856-4da7-c99d-b886f161b16f"
   },
   "outputs": [],
   "source": [
    "from pycbc.catalog import Merger\n",
    "from pycbc import psd, frame, filter as filt\n",
    "from astropy.utils.data import download_file\n",
    "\n",
    "m = Merger(\"GW170817\")\n",
    "\n",
    "# List of observatories we'll analyze\n",
    "ifos = [\"H1\", \"L1\", \"V1\"]\n",
    "\n",
    "# we'll keep track of the filename locations as we'll need them later\n",
    "data_filenames = {}\n",
    "\n",
    "# The single template waveform model needs these data products\n",
    "psds = {}\n",
    "data = {}\n",
    "channel_name = \"{}:LOSC-STRAIN\"\n",
    "\n",
    "for ifo in ifos:\n",
    "    print(\"Processing {} data\".format(ifo))\n",
    "    \n",
    "    # Download the gravitational wave data for GW170817\n",
    "    # Note: The GWOSC frames feature a huge glitch in L1 that significantly\n",
    "    #       impacts parameter inference. We will use \"cleaned\" frames here,\n",
    "    #       which have had the glitch removed.\n",
    "    #url = \"https://www.gw-openscience.org/eventapi/html/GWTC-1-confident/GW170817/v3/{}-{}_GWOSC_4KHZ_R1-1187006835-4096.gwf\"\n",
    "    url = \"https://dcc.ligo.org/public/0146/P1700349/001/{}-{}_LOSC_CLN_4_V1-1187007040-2048.gwf\"\n",
    "    fname = download_file(url.format(ifo[0], ifo), cache=\"update\")\n",
    "    data_filenames[ifo] = fname\n",
    "    \n",
    "    # Read the gravitational wave data and do some minimal\n",
    "    # conditioning of the data.\n",
    "    ts = frame.read_frame(fname, channel_name.format(ifo),\n",
    "                          start_time=int(m.time - 260),\n",
    "                          end_time=int(m.time + 40))\n",
    "    ts = filt.highpass(ts, 15.0)                     # Remove low frequency content\n",
    "    ts = filt.resample_to_delta_t(ts, 1.0/2048)      # Resample data to 2048 Hz\n",
    "    ts = ts.time_slice(m.time-112, m.time + 16) # Limit to times around the signal\n",
    "    data[ifo] = ts.to_frequencyseries()         # Convert to a frequency series by taking the data's FFT\n",
    "\n",
    "    # Estimate the power spectral density of the data\n",
    "    PSD = psd.interpolate(ts.psd(4), ts.delta_f)\n",
    "    PSD = psd.inverse_spectrum_truncation(PSD, int(4 * PSD.sample_rate), \n",
    "                                          trunc_method='hann',\n",
    "                                          low_frequency_cutoff=20.0)\n",
    "    psds[ifo] = PSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXHMHZjIjmGH"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "A number of parameters must also be provided as 'static' parameters. These include \n",
    "   * mass1 (Units in solar masses)\n",
    "   * mass2 (Units in solar masses)\n",
    "   * f_lower (used for where to start our gravitational waveform model generation from)\n",
    "   * approximant (This sets which waveform model we are using)\n",
    "\n",
    "If a model supports other intrinsic parameters (such as components spins), they may \n",
    "also optionally be provided.\n",
    "\n",
    "There are also a fixed set of 'variable' parameters. These are the only ones which we\n",
    "can obtain estimates of with this model. These are\n",
    "   * ra        (Units in radians)\n",
    "   * dec       (Units in radians)\n",
    "   * distance  (Units in Megaparsecs)\n",
    "   * inclination (Units in radians)\n",
    "   * polarization (Units in radians)\n",
    "   * tc           (Units in seconds): This parameter is roughly the time of merger.\n",
    "   \n",
    "It's important to note that anything which could be a variable paramater, can be transformed\n",
    "into a static parameter by supplying a specific value for it. We take advantage of this below\n",
    "to limit our analyis to only sample over 'distance', 'inclination', and 'tc'. We set the sky location\n",
    "to the location of NGC 4993, the galaxy where an electromagnetic counterpart to GW170817 was observed. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtVKVS1ijmGK"
   },
   "outputs": [],
   "source": [
    "# Parameters to keep fixed\n",
    "static = {'mass1':1.3757,\n",
    "          'mass2':1.3757,\n",
    "          'approximant':\"TaylorF2\",\n",
    "          'f_lower':25.0,\n",
    "          'polarization':0,\n",
    "          'ra': 3.44615914,\n",
    "          'dec': -0.40808407\n",
    "         }\n",
    "\n",
    "# Parameters to vary\n",
    "variable = ('distance',\n",
    "            'inclination',\n",
    "            'tc')\n",
    "\n",
    "# Set priors\n",
    "inclination_prior = dists.SinAngle(inclination=None)\n",
    "distance_prior = dists.Uniform(distance=(10, 100))\n",
    "tc_prior = dists.Uniform(tc=(m.time-0.1, m.time+0.1))\n",
    "\n",
    "prior = dists.JointDistribution(variable, inclination_prior, distance_prior,\n",
    "                                tc_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tjRu3RSjmGL"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "We are not ready to create our SingleTemplate model instance. Note how the variable and static parameters\n",
    "are passed to the model. This is a common way this information can be passed for built-in pycbc inference models.\n",
    "\n",
    "Notice that we are no longer using the Emcee sampler. While Emcee is sufficient for many problems, EmceePT, a parallel tempered version of Emcee is more effective at most gravitational-wave data analysis problems. There is one additional parameter we need to give to EmcceePT which is the number of temperatures. The output of this sampler will thus be 3-dimensional (temps x walkers x iterations). The 'coldest' temperature (0) will contain our actual results. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSphAuk_jmGO"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "f_lows = {ifo: 25.0 for ifo in ifos}\n",
    "datacopy = copy.deepcopy(data)\n",
    "model = models.SingleTemplate(variable, datacopy, f_lows, static_params=static,\n",
    "                              psds=psds, prior=prior, sample_rate=8192)\n",
    "\n",
    "engine = sampler.EmceePTSampler(model, 3, 200, nprocesses=8)\n",
    "_ = engine.set_p0() # If we don't set p0, it will use the models prior to draw initial points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iseSEHkhjmGQ"
   },
   "outputs": [],
   "source": [
    "# Note it may take ~1-3 minutes for this to run\n",
    "engine.run_mcmc(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxV68ymUjmGR"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">  In addition to the sampled parameters, we can also get the likelihood values our model produces. We don't go into it here, but it is also possible for models to make arbitrary auxiliary information about each sample available. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "mbHNuw2wjmGS",
    "outputId": "423dc70a-2060-4972-9800-d3ae7bbf5809"
   },
   "outputs": [],
   "source": [
    "loglike = engine.model_stats['loglikelihood']\n",
    "samps = engine.samples\n",
    "\n",
    "# Note how we have to access the arrays differently that before since there is an additional dimension. \n",
    "# The zeroth element of that dimension represents the 'coldest' and is the one we want for our results.\n",
    "# The other temperatures represent a modified form of the likelihood that allows walkers to traverse\n",
    "# the space more freely.\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "p = ax.scatter(samps['distance'][0,:,-1],\n",
    "               samps['inclination'][0,:,-1],\n",
    "               c=loglike[0,:,-1])\n",
    "ax.set_xlabel('Distance (Mpc)')\n",
    "ax.set_ylabel('Inclination (Radians)')\n",
    "\n",
    "c = plt.colorbar(p)\n",
    "c.set_label('Loglikelihood')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "bAcDiAd-jmGT",
    "outputId": "2ca3e2fe-061b-4906-ba2e-9ce2b9981b44"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.hist(samps['distance'][0,:,-1].flatten(), bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBqSFutmjmGU"
   },
   "source": [
    "#### 2.1 Watch how the position of the walkers evolves in time ###\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "We can see how our ensemble of walkers evolves with time using the animation utilities of matplotlib. We haven't covered the concept of \"burn-in\" in this tutorial, however, if you watch the animation, you can see the point\n",
    "that the distribution is effectively burned-in. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "HHawTVqsjmGU",
    "outputId": "8796de3c-6dc2-4ca7-c927-5b2e7bd68812"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "\n",
    "# We'll plot the initial position of the walkers\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "a = ax.scatter(samps['distance'][0,:,0],\n",
    "               samps['inclination'][0,:,0],\n",
    "               c=loglike[0,:,0])\n",
    "ax.set_xlabel('Distance (Megaparsecs)')\n",
    "ax.set_ylabel('Inclination (radians)')\n",
    "c = plt.colorbar(a)\n",
    "c.set_label('Loglikelihood')\n",
    "\n",
    "# This function will update the plot with the ith iteration of our mcmc chain.\n",
    "def animate(i):\n",
    "    dat = np.array([samps['distance'][0,:,i], samps['inclination'][0,:,i]])\n",
    "    a.set_offsets(dat.T)\n",
    "    a.set_array(loglike[0,:,i])\n",
    "    a.set_clim(vmin=min(loglike[0,:,i]), vmax=max(loglike[0,:,i]))\n",
    "    return (a, )\n",
    "    \n",
    "nsamples = len(samps['distance'][0,0,:])\n",
    "ani = animation.FuncAnimation(fig, animate, frames=nsamples,\n",
    "                              interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "U2b_p9EMjmGV",
    "outputId": "f5112b58-d93f-4ef2-d454-ad29d3b348d7"
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import PillowWriter\n",
    "from IPython.display import Image\n",
    "\n",
    "# Note to get this to play, you may need to right click on the image and\n",
    "# download to your computer or open the image in a new tab of your browser\n",
    "ani.save('move.gif', writer=PillowWriter(fps=5))\n",
    "with open('move.gif','rb') as f:\n",
    "    display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcYpFlrIjmGV"
   },
   "source": [
    "####  Suggested Problems ####\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "    \n",
    "1. If we exclude Virgo and Livingston how is the recovery of distance and inclination affected? How about if we just exclude Virgo? Hint: you can do this by modifying the 'ifos' list of the first cell in this section. \n",
    "2. GW170817 was identified with a counterpart in NGC4993. If we use the distance to NG4993 as a prior (41 +-3 Mpc) can we improve our estimate of the source binary inclination? Try setting a Gaussian prior based on this additional information.\n",
    "3. How well measured is the time of coalesence ('tc') ?\n",
    "3. (For the curious) Try the EmceeSampler like we've used in previous examples. How do the results compare after the same number of iterations? Note, remember that the Emcee and EmceePT samplers don't have the same format sample chains. Ecmee uses a walker x iteration chain, while EmceePT will have a temp x walker x iteration format.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfCfGAQBjmGV"
   },
   "source": [
    "## 3. Using the `pycbc_inference` command-line script\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "The examples above illustrated how the models and samplers are used in the inference module. While this is useful for quick checks, it's not feasible to do large-scale parameter estimation problems in a jupyter notebook. Instead, when actually doing parameter estimation, we use the command-line script `pycbc_inference`. This pulls together the sampler, models, and distributions modules so that we may perform Bayesian inference.\n",
    "\n",
    "The key arguments that `pycbc_inference` takes are an output file and one or more `config-files` (run `pycbc_inference --help` to see all of the arguments it takes). The config files entirely specify the problem that you will be analyzing. In them, you specify the model to use, the variable parameters, and the prior on each of the parameters. If the model involves gravitational-wave data, you also specify what data to load and how to estimate the PSD. To see how this works, let's repeat the above analysis where we estimate the distance and inclination of GW170817 using the `SingleTempate` model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfCfGAQBjmGV"
   },
   "source": [
    "### 3.1 Setting up the config file ###\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "*Note: in order to do everything from within the notebook, we'll create the config files by echoing a python string to  a file. In a normal situation, you would just use your favorite text editor to create the config files.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfCfGAQBjmGV"
   },
   "source": [
    "#### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcKrGUcejmGW"
   },
   "outputs": [],
   "source": [
    "model_config = \"\"\"\n",
    "[model]\n",
    "name = single_template\n",
    "low-frequency-cutoff = 25.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwZQAtVqjmGX",
    "outputId": "23da4e85-e62d-4224-8de5-73cb67460f92"
   },
   "outputs": [],
   "source": [
    "!echo '{model_config}' > model.ini\n",
    "!cat model.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooDUAmdijmGX"
   },
   "source": [
    "#### The data\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "    \n",
    "Since this model uses data, we also need to create a data section that specifies what data to read for the analysis and the PSD estimation. See the [PyCBC Inference docs](https://pycbc.org/pycbc/latest/html/inference.html#setting-data) for more information about what options are available and what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTdhOuemjmGY"
   },
   "outputs": [],
   "source": [
    "data_config = \"\"\"\n",
    "[data]\n",
    "instruments = H1 L1 V1\n",
    "trigger-time = {event_tc}\n",
    "analysis-start-time = -260\n",
    "analysis-end-time = 40\n",
    "psd-estimation = median\n",
    "psd-start-time = -260\n",
    "psd-end-time = 40\n",
    "psd-segment-length = 4\n",
    "psd-segment-stride = 2\n",
    "psd-inverse-length = 4\n",
    "strain-high-pass = 15\n",
    "pad-data = 8\n",
    "sample-rate = 2048\n",
    "frame-files = H1:{h1file} L1:{l1file} V1:{v1file}\n",
    "channel-name = H1:{chan} L1:{chan} V1:{chan}\n",
    "\"\"\".format(event_tc=Merger(\"GW170817\").time,\n",
    "           h1file=data_filenames['H1'],\n",
    "           l1file=data_filenames['L1'],\n",
    "           v1file=data_filenames['V1'],\n",
    "           chan=channel_name[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAnuc_d2jmGa",
    "outputId": "3bd5a52a-af90-40e6-99f4-6ce7cbf34670"
   },
   "outputs": [],
   "source": [
    "!echo '{data_config}' > data.ini\n",
    "!cat data.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Cr5BLYjmGc"
   },
   "source": [
    "#### The prior\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Now let's specify the prior. We need to provide a section that lists the variable parameters and another that specifies the static parameters. For every variable parameter we have to provide one or more `prior` section(s) that specifies the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnMvdUb7jmGd"
   },
   "outputs": [],
   "source": [
    "prior_config = \"\"\"\n",
    "[variable_params]\n",
    "distance =\n",
    "inclination =\n",
    "delta_tc =\n",
    "\n",
    "[static_params]\n",
    "mass1 = 1.3757\n",
    "mass2 = 1.3757\n",
    "f_lower = 25.0\n",
    "approximant = TaylorF2\n",
    "polarization = 0\n",
    "ra = 3.44615914\n",
    "dec = -0.40808407\n",
    "\n",
    "[prior-distance]\n",
    "name = uniform\n",
    "min-distance = 10\n",
    "max-distance = 100\n",
    "\n",
    "[prior-inclination]\n",
    "name = sin_angle\n",
    "\n",
    "[prior-delta_tc]\n",
    "name = uniform\n",
    "min-delta_tc = -0.1\n",
    "max-delta_tc = 0.1\n",
    "\n",
    "[waveform_transforms-tc]\n",
    "name = custom\n",
    "inputs = delta_tc\n",
    "tc = ${data|trigger-time} + delta_tc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onndUM75jmGf",
    "outputId": "170dc4ad-867c-4fbf-ac94-45eb31dcb224"
   },
   "outputs": [],
   "source": [
    "!echo '{prior_config}' > prior.ini\n",
    "!cat prior.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJLfhDDfjmHL"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    " *But wait!* There's something odd here: the list of variable parameters contains a `delta_tc`, whereas we need to provide a `tc` for the coalescence time. This illustrates that the `variable_params` can be anything we like them to be with any name (we could have called `delta_tc` `foobar` if we liked). They do not need to be predefined in code anywhere. However, the waveform models do need specific parameters to be provided in order to generate a template waveform. If we use a parameter that is not recognized by the waveform model (see [here](https://pycbc.org/pycbc/latest/html/inference.html#waveform-transforms) for a list of known parameters), then we must provide a `waveform_transforms` section that tells the code how to convert that parameter into something that the waveform model recognizes. In this case, `delta_tc` is not recognized by the waveform model, but `tc` is. So, we've provided a section that converts `delta_tc` into `tc` by adding the trigger time. Also note that when providing the trigger time we used `${data|trigger-time}`. We can use variable substitution in config files. In this case, we've told the config file to get the trigger time that's stored in the `[data]` section. This illustrates why we've chosen to make `delta_tc` the variable parameter: we do not need to copy and paste hard-to-read GPS times all over the place. Instead, we only need provide a time in one location, then refer to it elsewhere. This makes it clear and easy to specify the tc prior, which is just a uniform window +/- 0.1s around the estimated time. Another advantage of this is we could analyze other events using the same prior file, just swapping out data files.\n",
    "\n",
    "We will see some other, more advanced uses of waveform transforms below. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOpnwpmajmHL"
   },
   "source": [
    "#### The sampler\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Finally, we need to specify the sampler settings. To do that, we need to provide a `sampler` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ou1gT5T5jmHV"
   },
   "outputs": [],
   "source": [
    "sampler_config = \"\"\"\n",
    "[sampler]\n",
    "name = emcee_pt\n",
    "ntemps = 3\n",
    "nwalkers = 200\n",
    "niterations = 400\n",
    "max-samples-per-chain = 1000\n",
    "\n",
    "[sampler-burn_in]\n",
    "burn-in-test = halfchain\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lsUarMFjmHX",
    "outputId": "970984eb-b70a-4dc3-cdd9-5324037d5e99"
   },
   "outputs": [],
   "source": [
    "!echo '{sampler_config}' > sampler.ini\n",
    "!cat sampler.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK19elKHjmHX"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Note that we've also provided a burn-in test. Here, we're telling it just use the second half of the chain. This means that at the end of the run, it will consider the second half of the chains to be post-burn in. An autocorrelation time will be computed over this and a posterior extracted. More sophisticated burn-in tests are available; see the [pycbc.inference.burn_in module](https://pycbc.org/pycbc/latest/html/pycbc.inference.html#module-pycbc.inference.burn_in) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlth6kyQjmHX"
   },
   "source": [
    "### 3.2 Run `pycbc_inference`\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Now that we've set up our config files, we can run `pycbc_inference`. You can either run it directly from the notebook as in the following block or open a terminal and run the command there: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FhEtFILjmHX",
    "outputId": "5a8f12f8-0846-4b07-ba0f-56d041914104"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference --verbose \\\n",
    "    --config-files model.ini data.ini prior.ini sampler.ini \\\n",
    "    --output-file inference.hdf \\\n",
    "    --seed 28572013 \\\n",
    "    --nprocesses 1 \\\n",
    "    --force    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4sJwxfrjmHY"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Now that it's done, we can see that an `inference.hdf` file exists in our directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHjpCd9WjmHZ",
    "outputId": "6f39f44e-9a83-4016-e6f5-47dc4c761333"
   },
   "outputs": [],
   "source": [
    "!ls -lh inference.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGhFgi9hjmHa"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "This contains all of the samples, and the information needed to extract the posterior from it, along with other diagnostic information about the sampler. Below, we show how to plot a posterior from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOpi8Q8IjmHa"
   },
   "source": [
    "## 4. Plotting the posterior using `pycbc_inference_plot_posterior`\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "The command-line tool `pycbc_inference_plot_posterior` is used to make corner and posterior plots of our results. We can give it either the direct output of `pycbc_inference` (here, `inference.hdf`) or a posterior file created using `pycbc_inference_extract_samples`. There are several plotting configuration options available (see `pycbc_inference_plot_posterior --help` for the list of options). Here we'll illustrate a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUex5QBnjmHa"
   },
   "source": [
    "### 4.1 Corner scatter plot\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Let's create a corner plot of our results. We'll show every sample in our posterior colored by the signal-to-noise ratio (here defined as $\\sqrt{2\\mathcal{L}}$ where $\\mathcal{L}$ is the log likelihood ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxYjdEfUjmHa",
    "outputId": "f54595ba-4e99-41f1-ecd7-30982792dc70"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --verbose \\\n",
    "    --input-file inference.hdf \\\n",
    "    --output-file posterior.png \\\n",
    "    --plot-scatter --plot-marginal --z-arg snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "dxQCoSvojmHb",
    "outputId": "1c5c1ecc-645a-473d-d1f7-875a16e115f9"
   },
   "outputs": [],
   "source": [
    "Image('posterior.png', height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEeHoQfkjmHd"
   },
   "source": [
    "### 4.2 2D density plot\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "By default, we see that it has plotted all of the `variable_params`. Let's just plot distance vs inclination instead. We can do that via the `--parameters` argument. Let's also make a density plot with 2D marginal contours rather than a scatter plot. We'll do that by dropping the `--plot-scatter` and `--z-arg` commands, adding `--plot-density` and `--plot-contours` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7l9zQnzGjmHe",
    "outputId": "0b24819d-6961-4880-f9a0-7219ae1a2185"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --verbose \\\n",
    "    --input-file inference.hdf \\\n",
    "    --output-file posterior-dist_inc.png \\\n",
    "    --plot-density --plot-contours --plot-marginal \\\n",
    "    --parameters inclination distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "AlBBjC99jmHe",
    "outputId": "79371161-c2c5-4931-c424-934097cd66e6"
   },
   "outputs": [],
   "source": [
    "Image('posterior-dist_inc.png', height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmIRUDwLjmHg"
   },
   "source": [
    "### 4.3 Manipulating parameters on the command line\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "The `--parameters` can be used to do more than just turn certain parameters on or off. We can also apply math functions to parameters using standard python syntax. Any numpy function may be used. Functions from the  [pycbc.conversions](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.conversions), [pycbc.coordinates](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.coordinates), or [pycbc.cosmology](http://pycbc.org/pycbc/latest/html/pycbc.html#module-pycbc.cosmology) modules are also available. To see all of the parameters in our results file, along with all the functions that may be carried out on them, we can use the `--file-help` argument. This will print a message to screen then exit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yxjQcUgjmHg",
    "outputId": "942e85ee-fda6-4a51-c801-3c1521c48564"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --input-file inference.hdf --file-help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXHN_DgTjmHg"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "One of the available functions, [redshift](https://pycbc.org/pycbc/latest/html/pycbc.html#pycbc.cosmology.redshift), takes in luminosity distance and converts it to redshift assuming a standard cosmology. Let's use that, and some of the numpy functions, to plot redshift versus inclination angle in degrees. Let's also add a line at the redshift of [NGC4993](https://en.wikipedia.org/wiki/NGC_4993) to see how well our results agree. We'll do that using the `--expected-parameters` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgJ8uG1ljmHr",
    "outputId": "252915ca-6a7a-4b22-b7fa-74f2c6a15bb7"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --verbose \\\n",
    "    --input-file inference.hdf \\\n",
    "    --output-file posterior-redshift_inc.png \\\n",
    "    --plot-density --plot-contours --plot-marginal \\\n",
    "    --parameters 'inclination*180/pi:$\\iota$ (deg)' 'redshift(distance):redshift' \\\n",
    "    --expected-parameters 'redshift(distance):0.009727'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "1IhOtnBTjmHs",
    "outputId": "06e02591-b6bc-4695-8695-55b3addc041e"
   },
   "outputs": [],
   "source": [
    "Image('posterior-redshift_inc.png', height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgarWmf8jmHs"
   },
   "source": [
    "## 5. Using waveform transforms in the config file\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "The functions available to the `--parameters` argument of `pycbc_inference_plot_posterior` are actually used throughout the `inference` module (if you're curious, this is made possible via [FieldArrays](https://pycbc.org/pycbc/latest/html/pycbc.io.html#pycbc.io.record.FieldArray), which is a custom wrapping of numpy record arrays; see the `inference_5_results_io` notebook for more details). Of particular note, you can use the same set of functions in the `waveform_transforms` section(s) of the config file. This allows you to carry out more complicated inference without needing to modify the PyCBC source code.\n",
    "\n",
    "To illustrate this, let's modify the prior in the above problem to use a prior uniform in comoving volume rather than a prior uniform in distance.\n",
    "\n",
    "First, we'll need boundaries for our comoving volume prior. Let's use the cosmology module to get the comoving volume corresponding to a luminosity distance of 10 and 100 Mpc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tatIAZyOjmHt",
    "outputId": "9702aff6-5a28-44a5-d512-479306b14ab7"
   },
   "outputs": [],
   "source": [
    "from pycbc import cosmology\n",
    "\n",
    "vmin, vmax = cosmology.cosmological_quantity_from_redshift(cosmology.redshift([10., 100.]), 'comoving_volume')\n",
    "print(vmin, vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0fiOzSLjmHu"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "Now let's modify our prior. We'll swap `distance` for `comoving_volume`. Since the waveform's require luminosity distance, we'll provide a waveform transform that uses the [distance_from_comoving_volume](https://pycbc.org/pycbc/latest/html/pycbc.html#pycbc.cosmology.distance_from_comoving_volume) function in the cosmology module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCRKXFeZjmHu"
   },
   "outputs": [],
   "source": [
    "prior_config = \"\"\"\n",
    "[variable_params]\n",
    "comoving_volume =\n",
    "inclination =\n",
    "delta_tc =\n",
    "\n",
    "[static_params]\n",
    "mass1 = 1.3757\n",
    "mass2 = 1.3757\n",
    "f_lower = 25.0\n",
    "approximant = TaylorF2\n",
    "polarization = 0\n",
    "ra = 3.44615914\n",
    "dec = -0.40808407\n",
    "\n",
    "[prior-comoving_volume]\n",
    "name = uniform\n",
    "min-comoving_volume = 4160\n",
    "max-comoving_volume = 3921536\n",
    "\n",
    "[waveform_transforms-distance]\n",
    "name = custom\n",
    "inputs = comoving_volume\n",
    "distance = distance_from_comoving_volume(comoving_volume)\n",
    "\n",
    "[prior-inclination]\n",
    "name = sin_angle\n",
    "\n",
    "[prior-delta_tc]\n",
    "name = uniform\n",
    "min-delta_tc = -0.1\n",
    "max-delta_tc = 0.1\n",
    "\n",
    "[waveform_transforms-tc]\n",
    "name = custom\n",
    "inputs = delta_tc\n",
    "tc = ${data|trigger-time} + delta_tc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txHXSIXZjmHu",
    "outputId": "d5e6fdac-6343-447b-b936-47f2ae485491"
   },
   "outputs": [],
   "source": [
    "!echo '{prior_config}' > prior.ini\n",
    "!cat prior.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE_pOk_pjmHv"
   },
   "source": [
    "### Suggested problems:\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "    \n",
    "    \n",
    " 1. Rerun the above script (starting from section 3.2) with the uniform in comoving volume prior. Note that when you plot the posterior, you'll get `comoving_volume` if you do not provide a `--parameters` argument. Add the parameters argument to plot distance instead. Do the same for redshift.\n",
    " 2. Repeat, but estimate the sky location. To do this, you'll want to move `ra` and `dec` into the `[variable_params]`. What prior should you use? Take a look at the [list of available distributions](https://pycbc.org/pycbc/latest/html/inference.html#configuring-the-prior). Answer [here](https://github.com/gwastro/pycbc/blob/v1.16.12/examples/inference/priors/bbh-uniform_comoving_volume.ini#L160-L161) (no peaking!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSWa2SQnjmHw"
   },
   "source": [
    "## 6. Estimate the masses of GW170817 ##\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "    \n",
    "In the previous example we quickly estimated the extrinsic parameters of GW170817 by fixing the intrinsic. This was fast, since it did not require regenerating a waveform for each likelihood evaluation. Now let's try estimating the masses of GW170817. This is generally slower, since it requires generating a waveform on each likelihood call. However, in the example below, we'll use the [relative binning model](https://pycbc.org/pycbc/latest/html/pycbc.inference.models.html#pycbc.inference.models.relbin.Relative). This uses a technique presented in [Zackay et al.](https://arxiv.org/abs/1806.08792). Basically, a reference waveform is used that's close to the peak likelihood. A linear approximation is used to interpolate the likelihood around this waveform, reducing the number of frequency points that we need to evaluate, and speeding up the analysis.\n",
    "\n",
    "To do this, we'll use the same `data.ini` and `sampler.ini` files as above. We'll change the `model` to the `relative` one and provide the necessary arguments to generate the fiducial waveform. We'll make the chirp mass (`mchirp`) and symmetric mass ratio (`eta`) the variable parameters; for speed, we'll fix the extrinsic parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3PPVjHLjmHw"
   },
   "source": [
    "### 6.1 Get the extrinsic parameters\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "For speed, we'll fix the extrinsic parameters. We'll use our previous results using the `SingleTemplate` model to get the maximum likelihood values of the distance, inclination, and coalescence time. To do that, we'll use `pycbc_inference_table_summary` to print out a table of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6GTRVnIjmHw",
    "outputId": "fb10de5c-bde2-4cd8-ba57-af63b4a404dc"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference_table_summary \\\n",
    "    --input-file inference.hdf \\\n",
    "    --output-file posterior_summary.html \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "qwX15ZJRjmHw",
    "outputId": "fa20896a-b662-439f-9960-bd9238fd398e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('posterior_summary.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTU26dPhjmHx"
   },
   "source": [
    "### 6.1 Setup the config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InMkn8rcjmHx"
   },
   "outputs": [],
   "source": [
    "model_config = \"\"\"\n",
    "[model]\n",
    "name = relative\n",
    "low-frequency-cutoff = 25.0\n",
    "high-frequency-cutoff = 1024.0\n",
    "epsilon = 0.03\n",
    "mass1_ref = 1.3757\n",
    "mass2_ref = 1.3757\n",
    "tc_ref = ${data|trigger-time}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMjk-zRQjmHy",
    "outputId": "bc2058ae-0cec-4511-8867-13aa4886c12d"
   },
   "outputs": [],
   "source": [
    "!echo '{model_config}' > model.ini\n",
    "!cat model.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1ORZO2AjmHz"
   },
   "outputs": [],
   "source": [
    "prior_config = \"\"\"\n",
    "[variable_params]\n",
    "mass1 =\n",
    "mass2 =\n",
    "\n",
    "[static_params]\n",
    "f_lower = 25.0\n",
    "approximant = TaylorF2\n",
    "polarization = 0\n",
    "ra = 3.44615914\n",
    "dec = -0.40808407\n",
    "distance = 48\n",
    "inclination = 3\n",
    "delta_tc = 0.028365\n",
    "\n",
    "[prior-mass1]\n",
    "name = uniform\n",
    "min-mass1 = 1\n",
    "max-mass1 = 2\n",
    "\n",
    "[prior-mass2]\n",
    "name = uniform\n",
    "min-mass2 = 1\n",
    "max-mass2 = 2\n",
    "\n",
    "[waveform_transforms-tc]\n",
    "name = custom\n",
    "inputs = delta_tc\n",
    "tc = ${data|trigger-time} + delta_tc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7V-zu6GjmH1"
   },
   "outputs": [],
   "source": [
    "prior_config = \"\"\"\n",
    "[variable_params]\n",
    "mchirp =\n",
    "eta =\n",
    "\n",
    "[static_params]\n",
    "f_lower = 25.0\n",
    "approximant = TaylorF2\n",
    "polarization = 0\n",
    "ra = 3.44615914\n",
    "dec = -0.40808407\n",
    "distance = 48\n",
    "inclination = 3.0\n",
    "delta_tc = 0.028365\n",
    "\n",
    "[prior-mchirp]\n",
    "name = uniform\n",
    "min-mchirp = 1.1876\n",
    "max-mchirp = 1.2076\n",
    "\n",
    "[prior-eta]\n",
    "name = uniform\n",
    "min-eta = 0.23\n",
    "max-eta = 0.25\n",
    "\n",
    "[waveform_transforms-mass1+mass2]\n",
    "; transform from mchirp, eta to mass1, mass2 for waveform generation\n",
    "name = mchirp_eta_to_mass1_mass2\n",
    "\n",
    "[waveform_transforms-tc]\n",
    "name = custom\n",
    "inputs = delta_tc\n",
    "tc = ${data|trigger-time} + delta_tc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RzKv9cOjmH2",
    "outputId": "7a20b07a-f04e-45f2-c7ed-2aa4ee390a97"
   },
   "outputs": [],
   "source": [
    "!echo '{prior_config}' > prior.ini\n",
    "!cat prior.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QNjJC4djmH4"
   },
   "source": [
    "### 6.2 Run\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    \n",
    "    \n",
    "*Note: this may take several minutes to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ge06jfFhjmH5",
    "outputId": "721c65db-14fe-4147-a138-ac1f24e7a30a"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference --verbose \\\n",
    "    --config-files model.ini data.ini prior.ini sampler.ini \\\n",
    "    --output-file inference-masses.hdf \\\n",
    "    --seed 3214897 \\\n",
    "    --nprocesses 1 \\\n",
    "    --force    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fEECnBcjmH5"
   },
   "source": [
    "### 6.3 Plot the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4W_1BxnmjmH6",
    "outputId": "6d0ad813-d3ba-4a29-dcc9-641ccb456945"
   },
   "outputs": [],
   "source": [
    "!pycbc_inference_plot_posterior --verbose \\\n",
    "    --input-file inference-masses.hdf \\\n",
    "    --output-file posterior.png \\\n",
    "    --plot-scatter --plot-marginal --z-arg snr \\\n",
    "    --parameters mchirp eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "7tyJV1hajmH6",
    "outputId": "a30a7fdf-f1d6-4b1d-aa1e-a093be91bf93"
   },
   "outputs": [],
   "source": [
    "Image('posterior.png', height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ4ElrLNjmH7"
   },
   "source": [
    "### Suggested problems:\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "\n",
    " 1. Plot source-frame chirp mass instead.\n",
    " 2. Repeat the above, but use a prior uniform in mass1 and mass2. Do the results look converged after just 400 iterations? If not, try increasing the number of iterations. Also try increasing the number of temperatures. Note that this will take longer to analyze as a result. When plotting the results, notice that there are points where mass2 > mass1. Replot with mass1 > mass2. You can do that by using the `primary_mass` and `secondary_mass` functions.\n",
    " 3. Repeat the same, but use a prior uniform in source-frame masses. See the example BBH prior config [here](https://github.com/gwastro/pycbc/blob/v1.16.12/examples/inference/priors/bbh-uniform_comoving_volume.ini) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rKgf-2PsjmFU",
    "h_JjuLQWjmFh",
    "U_AzyF90jmFu",
    "avtlTgf8jmFw",
    "dcYpFlrIjmGV",
    "mE_pOk_pjmHv",
    "qZ4ElrLNjmH7"
   ],
   "name": "tutorial_2-inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
